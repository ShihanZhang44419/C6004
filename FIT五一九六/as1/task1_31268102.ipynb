{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assignment 1\n",
    "\n",
    "#### Student Name:  Shihan Zhang\n",
    "\n",
    "#### Student ID:        31268102\n",
    "\n",
    "Date: 09/11/2020\n",
    "\n",
    "Version: 3.0\n",
    "- Version 1: process sinlge file & write to .xml file\n",
    "- Version 2: added unicode Text processing\n",
    "- Version 3: looping files(processing single file and write to file)\n",
    "\n",
    "Environment: Python 3.6.0 and Anaconda 4.3.0 (64-bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The workflow of this task:\n",
    "Loop all the single source files\n",
    "\n",
    "Within the loop we do the process 1 & 2\n",
    " 1. process the single files\n",
    "  - use regex to capture id, date, text\n",
    "  - filter the none English tweets\n",
    " 2. write to .xml file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import langid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization the basic parameters\n",
    "- savePath(path for write to file)\n",
    "- xml format\n",
    "- `allTweetDict` -- dictionary for store all tweets value\n",
    "\n",
    "#### Structure of `allTweetDict`\n",
    " allTweetDict{\n",
    " - layer 1 key:   {date:'2020-03-22' \n",
    "   - layer 2 key:       {id:'1241576773457829889'\n",
    "    - value:                     {tweet:'@TheDeshBhakt'}}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# output file save path\n",
    "savedPath = '31268102.xml'\n",
    "\n",
    "# xml format\n",
    "xml_head = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<data>'''\n",
    "xml_tail = '''\\n</data>'''\n",
    "xml_block = '''\\n<tweets date=\"{date}\">{tweetList}\\n</tweets>'''\n",
    "xml_tweet = '''\\n<tweet id=\"{id}\">{text}</tweet>'''\n",
    "\n",
    "# Use dictionary format to store all data; xml can be converted with dictionary format;\n",
    "allTweetDict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create regex patterns\n",
    "- `id_pattern` for capture id data `\"id\":\"(\\d*)\"`\n",
    "  - the regex `\"id\":`capture the string after \"id:\"\n",
    "  - the regex `\"()\"`look for the group of the collection between `\" \"`\n",
    "  - the regex `\\d*` capture the zero of more digits, can also use `\\d+`\n",
    "  combine above regex together we got returned digits between `\" \"` and after the `\"id:\"`\n",
    "  \n",
    "- `text_pattern` for capture text data `\"text\":\"([^\"]*)\"`\n",
    "   - the regex `\"text\":`capture the string after \"text:\"\n",
    "   - the regex `\"()\"`look for the group of the collection between `\" \"`\n",
    "   - the regex `[^\"]*` capture all the string before `\"`, with `*` this condition can repret multiple times\n",
    "   combine above regex together we got the return string(all type) between `\" \"` and after `\"text:\"`\n",
    "   \n",
    "- `date_pattern` for capture date data `\"created_at\":\"(\\d{4}-\\d{2}-\\d{2})`\n",
    "   - the regex `\"created_at:\"`capture the string after \"created:\"\n",
    "   - the regex `\"()\"`look for the group of the collection between `\" \"`\n",
    "   - the regex `\\d{4}-\\d{2}-\\d{2}` capture the digit with `-``\n",
    "     - `\\d{4}` digit repeat 4 times, `\\d{2}` digit repreat 2 times\n",
    "   combine above regex together we got the return date(4 digits after`-`, 2 digits after `-`, 2 digits after `-`) between `\" \"` and after `created_at:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id pattern\n",
    "id_pattern = re.compile('\\\"id\\\":\\\"(\\d*)\\\"')\n",
    "\n",
    "# text_pattern\n",
    "text_pattern = re.compile('\\\"text\\\":\\\"([^\\\"]*)\\\"')\n",
    "\n",
    "# date_pattern, only need year-month-day so filter out the exact time.\n",
    "date_pattern = re.compile('\\\"created_at\\\":\\\"(\\d{4}-\\d{2}-\\d{2})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load source file\n",
    "- With `encoding = 'utf -8'` to reading all files in the folder\n",
    "- Function for load source file. \n",
    "- Pass the 'filename' to function, function process the file, return the 'content' as the String value carry all text content of single file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename):\n",
    "    with open(f\"./31268102/{filename}\", 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the text content\n",
    "- This function is the main part of processing the tweets data.\n",
    "\n",
    "#### Explanation of list length check for merging tweets data.\n",
    "- To ensure that the three types of data (data, id, text) are merged in the correct position. Using `if-statement` to check based on the length of the list. If the list lengths of the three types of data are different, that measn there is data loss or index position mismatch. Then the tweets data will not be merged.\n",
    "\n",
    "#### Unicode Text processing\n",
    "- 1. Use`strip()` to detect the text, if tweet text only contain single`'\\'` then skip this tweet. If the tweet has more than one backlash. Then use`rstrip()` to reformat to a single backslash.\n",
    "\n",
    "\n",
    "#### Add new tweets to 'allTweetDict = {}'\n",
    "In the loop of the index, we have the\n",
    "- tweet_date as the `date_list[index]`,\n",
    "- tweet_text as the `text_list[index]`,\n",
    "- tweet_id as the `text_id[index]`\n",
    "\n",
    "Then we check whether this 'tweet_date' is in the global dictionary or not.\n",
    "- if the current date does exist in the dictionary:\n",
    "  - we append the tweet text under this date (this is the under same date case)\n",
    "- if not exist in the dictionary:\n",
    "  - we check whether this is the unique id\n",
    "  - only this id is unique in the dictionary\n",
    "    - we insert the current date with the value(current id and text) to the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_text(content):\n",
    "    # assign the dictionary as the global variable, to carry the value from all files data.\n",
    "    global allTweetDict\n",
    "    \n",
    "    # list stores the values captured by regex\n",
    "    id_list = re.findall(id_pattern, content)\n",
    "    text_list = re.findall(text_pattern, content)\n",
    "    date_list = re.findall(date_pattern, content)\n",
    "    \n",
    "    # remove duplicated date value\n",
    "    unique_date = set(date_list)\n",
    "    \n",
    "    ## list length check for merging tweets data.\n",
    "    if len(id_list) == len(text_list) == len(date_list):\n",
    "        # looping the index range of data list\n",
    "        for i in range(len(id_list)):\n",
    "            tweet_date = date_list[i]\n",
    "            tweet_id = id_list[i]\n",
    "            tweet_text = '{}'.format(text_list[i])\n",
    "            ## unicode Text processing\n",
    "            if tweet_text.strip() == '\\\\':\n",
    "                # jump to the next index if current text only contain single '\\'\n",
    "                return\n",
    "            # remove the 'right' backslash\n",
    "            tweet_text = tweet_text.rstrip('\\\\')\n",
    "            \n",
    "            # process the utf-16 surrogatespass\n",
    "            second_text = eval(repr(tweet_text).replace('\\\\\\\\', '\\\\').replace('\\\\n\\\\n', '\\\\n'))\n",
    "            res = second_text.encode('utf-16', 'surrogatepass').decode('utf-16')\n",
    "            # reaplce the single backslash to white sapce\n",
    "            res = res.replace('\\\\', '')\n",
    "\n",
    "            # use langid to filter none English tweets\n",
    "            if (langid.classify(res)[0]) == 'en':\n",
    "                # insert to dictionary if tweets_date is new\n",
    "                # prevent to insert dupilcated day content\n",
    "                if tweet_date not in allTweetDict:\n",
    "                    allTweetDict[tweet_date] = {tweet_id: res}\n",
    "                \n",
    "                # if tweets from different file has the same date\n",
    "                else:\n",
    "                    # assgin instant values(under date key) \n",
    "                    current_tweets = allTweetDict[tweet_date]\n",
    "                    # check if tweet id already exist in the allTweetDict\n",
    "                    # if not already in allTweetDict\n",
    "                    # then insert this tweets text under the current date with current id \n",
    "                    if tweet_id not in current_tweets:\n",
    "                        allTweetDict[tweet_date][tweet_id] = res\n",
    "            # else:\n",
    "            #     print NONE English tweets\n",
    "            #     print('====', tweet_text)\n",
    "    \n",
    "    # if list length check not pass\n",
    "    else:\n",
    "        print(\"lenght of lists are not equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to .xml file\n",
    "#### with following functions\n",
    "- `write_head()`\n",
    " - write xml format header tags\n",
    " \n",
    "- `write_file()`\n",
    " - wrtie tweets text content\n",
    " \n",
    "- `write_tail()`\n",
    " - write xml format footer tags\n",
    "\n",
    "##### Why use write function separately?\n",
    "Based on the overall program structure, I need to continuously write content to the txt file. \n",
    "But I only need to write the header tag and footer tag once. \n",
    "So I decided to make header and footer separate functions and call them only once.\n",
    "Between header and footer, I can repeatedly call the function to writing tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_head():\n",
    "    with open(savedPath, 'w', encoding='utf-8') as f:\n",
    "        f.write(xml_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(blockText):\n",
    "    # write to .xml file\n",
    "    with open(savedPath, 'a', encoding='utf-8') as f:\n",
    "        f.write(blockText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tail():\n",
    "    with open(savedPath, 'a', encoding='utf-8') as f:\n",
    "        f.write(xml_tail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function \n",
    "to Combine all sub-functions and process all files using loop\n",
    "#### wrtie main tweets content\n",
    "- First layer, use loop the dictionary with k,v\n",
    "  - use list `each_block` to reformat the tweets' content in second layer.\n",
    "  \n",
    "- Second layer, loop the tweets'text and id under the date\n",
    "  - use the `xml format` string to combine the data with xml tags.\n",
    "      - then append to `each_block` list\n",
    "  - join the elements in the `each_block` list by the tweets'date.\n",
    "      - then format the joined data with xml tags\n",
    "  - write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # append all file under the dirctory to folder list\n",
    "    folders = os.listdir(r'.\\31268102')\n",
    "\n",
    "    # loop all the files\n",
    "    for index, fileName in enumerate(folders):\n",
    "        if index > 3:\n",
    "            break\n",
    "        # print('=========', fileName, '====================')\n",
    "        # process the single file\n",
    "        originText = load_file(fileName)\n",
    "        process_text(originText)\n",
    "\n",
    "    # writting file\n",
    "    # write header tags\n",
    "    write_head()\n",
    "    \n",
    "    # wrtie main tweets content\n",
    "    for date, tweetList in allTweetDict.items():\n",
    "        each_block = []\n",
    "        for tweet_id, tweet_text in tweetList.items():\n",
    "            each_block.append(\n",
    "                xml_tweet.format(id=tweet_id, text=tweet_text)\n",
    "            )\n",
    "        date_block = xml_block.format(date=date, tweetList=''.join(each_block))\n",
    "        write_file(date_block)\n",
    "    \n",
    "    # wrtie footer tags\n",
    "    write_tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
