{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assignment 1\n",
    "\n",
    "#### Student Name:  Shihan Zhang\n",
    "\n",
    "#### Student ID:        31268102\n",
    "\n",
    "Date: 13/11/2020\n",
    "\n",
    "Version: 5.0\n",
    "\n",
    "Environment: Python 3.6.0 and Anaconda 4.3.0 (64-bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work Flow for This task\n",
    "\n",
    "1. First filter the text to remove non-English\n",
    "\n",
    "2. merg all text from the single sheet to a dictionary `new_dict`\n",
    "\n",
    "3. Use regx to grab words\n",
    "\n",
    "4. Use the value in `new_dict` to generate primary uni gram -> `raw_uni`\n",
    "\n",
    "5. De-duplicate words in `raw_uni` \"page break/single day\", and then calculate document frequncy\n",
    "\n",
    "6. Remove the token which has (60 > document frequncy > 5) in `raw_uni` then get `unigramList`\n",
    "    \n",
    "7. Process `raw_vocab` with stopword to remove stopword, then process tokens with`PorterStemmer()`. \n",
    "\n",
    "8. Select top 100 words (single day) -> `final_uni`then writte into `.txt`.\n",
    "\n",
    "9. Use `raw_vocab` to generate -> `raw_bigram`.. Find the top 100 biggram (single day) and write it to `.txt`\n",
    "\n",
    "10. Find the 200 meaningful bigrams with PIM measure, then add to the `fial_vocab`\n",
    "\n",
    "11. Generate matrix through `final_cleaned_vocab{}` and write vector to `.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import langid\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization the basic parameters\n",
    "- list `stopwords` store for stopword\n",
    "- `regexpStr` the provided regex pattern\n",
    "- `default_tokenizer` the regexToknizeer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init list\n",
    "stopwords_list = []\n",
    "# init tokenizer\n",
    "regexpStr = r\"[a-zA-Z]+(?:[-\\'][a-zA-Z]+)?\"\n",
    "default_tokenizer = RegexpTokenizer(regexpStr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load source file\n",
    " - use pandas library to load excel file\n",
    " - `stopwords_list` to store loaed stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel = pd.ExcelFile('./31268102.xlsx')\n",
    "with open(\"./stopwords_en.txt\", 'r') as f:\n",
    "    stopwords_list = f.read().splitlines()\n",
    "\n",
    "# remove the duplicated value for stopword \n",
    "stopwords_set = set(stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization the Component functions\n",
    " - pass data tp `sortFreDict()`to get the sorted dictionary value\n",
    " - pass data and savePath to `saved()` to wrtie `.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to sort all word for final output\n",
    "def sortFreqDist(data):\n",
    "    res = []\n",
    "    for key, value in data.items():\n",
    "        # print(key, value)\n",
    "        temp = (key, value)\n",
    "        res.append(temp)\n",
    "    # Sort elments by frequency\n",
    "    result = sorted(res, key=lambda x: int(x[1]), reverse=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saved(data: str, filePath, mode='w'):\n",
    "    with open(filePath, f'{mode}') as f:\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read single day/sheet \n",
    "from workflow:\n",
    "1. First filter the text to remove non-English\n",
    "2. merg all text from the single sheet to a dictionary `new_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init dictionary for store single sheet data\n",
    "new_dict = {}\n",
    "\n",
    "# loop the all the sheets in the excel file\n",
    "for i in range(len(excel.sheet_names)):\n",
    "    # instant list for store text\n",
    "    content = []\n",
    "    # parse sheet by index\n",
    "    df = excel.parse(i)\n",
    "    # Delete columns that contain all NaN values\n",
    "    df.dropna(1, how='all', inplace=True)\n",
    "    # Delete all rows with NaN values\n",
    "    df.dropna(axis=0, how='all', inplace=True)\n",
    "    # rename columns by first row in the df\n",
    "    df.rename(columns=df.iloc[0], inplace=True)\n",
    "    # drop the frist row\n",
    "    df = df[1:]\n",
    "    # reset row index\n",
    "    df.index = range(len(df.index))\n",
    "    \n",
    "    # filter out None English text\n",
    "    for k in range(len(df.values)):\n",
    "        # conver elemnt to string\n",
    "        text = str(df.values[k][0])\n",
    "        # detect if is English\n",
    "        if (langid.classify(text)[0]) == 'en':\n",
    "            # append English text to list\n",
    "            content.append(text)\n",
    "    \n",
    "    # join all the text value(from every single sheet) to dictionary\n",
    "    # the key for this dictionary is \"sheet name\"\n",
    "    new_dict[excel.sheet_names[i]] = '\\n'.join(map(str, content))\n",
    "\n",
    "# print(new_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find unigram\n",
    "from workflow:\n",
    "3. Use regexTokenizer to grab words\n",
    "4. Use the value in `new_dict` to generate primary uni gram -> `raw_uni`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unigram(data):\n",
    "    # init the dictionary to store raw unigram\n",
    "    raw_uni = {}\n",
    "    # loop the input dictionary key\n",
    "    for i in data.keys():\n",
    "        # extract the text content\n",
    "        words = data[i]\n",
    "        # Use tokenizer to capture words and convert them to lowercase\n",
    "        unigram_token = default_tokenizer.tokenize(words.lower())\n",
    "        # Use this dictionary to integrate all unigrams of dates\n",
    "        # with date as Key and unigram token as value\n",
    "        raw_uni[i] = unigram_token\n",
    "\n",
    "    return raw_uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesee the clean unigram and vocabulary\n",
    "from workflow:\n",
    "5. De-duplicate words in `raw_uni` \"page break/single day\", and then calculate document frequncy\n",
    "\n",
    "6. Remove the token which has (60 > document frequncy > 5) in `raw_uni` then get `unigramList`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "p = PorterStemmer()\n",
    "# use un-proccess text to generate raw_vocab \n",
    "raw_vocab = find_unigram(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for generate clean vocabulary and clean unigram\n",
    "def unigramToVocab():\n",
    "    # Combine all words and remove duplicate words\n",
    "    # Only the daily words are deduplicated, and the merged words are not deduplicated\n",
    "    doc_words = list(chain.from_iterable([set(value) for value in raw_vocab.values()]))\n",
    "    # print('[doc_words]', len(doc_words))\n",
    "    \n",
    "    # Calculation frequency\n",
    "    single_doc_freq = FreqDist(doc_words)\n",
    "\n",
    "    # list to store the words that meet the conditions(document frequncy)\n",
    "    includedWords = []\n",
    "    \n",
    "    # find the document frequncy\n",
    "    # only keep the words that meet the threshold requirement\n",
    "    for word, freq in single_doc_freq.items():\n",
    "        if 5 <= freq <= 60:\n",
    "            includedWords.append(word)\n",
    "    \n",
    "    # dictionary to store the processed words\n",
    "    cleaned_vocab = {}\n",
    "    \n",
    "    for date, values in raw_vocab.items():\n",
    "        temp = []\n",
    "        for word in values:\n",
    "            # Remove stop words; filter data that dont meet threshold requirement\n",
    "            if word in includedWords and word not in stopwords_set:\n",
    "                new_word = p.stem(word)\n",
    "                # keep the words which length less than 3\n",
    "                if len(new_word) > 2:\n",
    "                    temp.append(word)\n",
    "        \n",
    "        # store the clean vocab to dictionary\n",
    "        cleaned_vocab[date] = temp\n",
    "    \n",
    "    # generated processed unigrame\n",
    "    unigramList = set(list(chain.from_iterable(cleaned_vocab.values())))\n",
    "    \n",
    "    return unigramList, cleaned_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting unigram text\n",
    "from workflow:\n",
    "7. Process `raw_vocab` with stopword to remove stopword, then process tokens with`PorterStemmer()`. \n",
    "\n",
    "8. Select top 100 words (single day) -> `final_uni`then writte into `.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genUniTxt():\n",
    "    path = './31268102_100uni.txt'\n",
    "    final_uni = ''\n",
    "    # get un-process unigram token\n",
    "    for date, words in raw_vocab.items():\n",
    "        each_words = []\n",
    "        for item in words:\n",
    "            # the raw word should not remove stopwords\n",
    "            if item in stopwords_set:\n",
    "                continue\n",
    "            # use prot stem words\n",
    "            new_word = p.stem(item)\n",
    "            if new_word not in stopwords_set:\n",
    "                each_words.append(new_word)\n",
    "        sorted_words = sortFreqDist(FreqDist(each_words))\n",
    "        # reformat all processed unigram to string, use date as the header\n",
    "        if not final_uni:\n",
    "            # first line\n",
    "            final_uni = str(date) + ':' + str(sorted_words[:100])\n",
    "        else:\n",
    "            final_uni = final_uni + '\\n' + str(date) + ':' + str(sorted_words[:100])\n",
    "    \n",
    "    # write to .txt file       \n",
    "    saved(final_uni, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting bigram text\n",
    "from workflow:\n",
    "9. Use `raw_vocab` to generate -> `raw_bigram` Find the top 100 biggram (single day) and write it to `.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genBiText():\n",
    "    path = './31268102_100bi.txt'\n",
    "    final_uni = ''\n",
    "    for date, words in raw_vocab.items():\n",
    "        # find bigram from raw_unit\n",
    "        raw_bigram = nltk.collocations.BigramCollocationFinder.from_words(words)\n",
    "        # sorted by frequncy\n",
    "        final_bigram = sortFreqDist(raw_bigram.ngram_fd)\n",
    "        \n",
    "        # reformat all processed bigram to string, use date as the header\n",
    "        if not final_uni:\n",
    "            # frist line\n",
    "            final_uni = str(date) + ':' + str(final_bigram[:100])\n",
    "        else:\n",
    "            final_uni = final_uni + '\\n' + str(date) + ':' + str(final_bigram[:100])\n",
    "    \n",
    "    # write to .txt file\n",
    "    saved(final_uni, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get 200 meaningful bigrams\n",
    "from workflow:\n",
    "10. Find the 200 meaningful bigrams with PIM measure, then add to the `fial_vocab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genVocab():\n",
    "    all_tweet_words = list(chain.from_iterable(raw_vocab.values()))\n",
    "    # find bigram from the raw unigram\n",
    "    raw_bigram = nltk.collocations.BigramCollocationFinder.from_words(all_tweet_words)\n",
    "    # get bigram with measures PMI 200\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    # get 200 meaningful words\n",
    "    top_200_bigrams = raw_bigram.nbest(bigram_measures.pmi, 200)\n",
    "    \n",
    "    # reformat the bigram with '_'\n",
    "    target_bigrams = []\n",
    "    for item in top_200_bigrams:\n",
    "        temp = item[0] + '_' + item[1]\n",
    "        target_bigrams.append(temp)\n",
    "    # return formated bigram\n",
    "    return target_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get Final vocabulary\n",
    "1. get vocabulary(unigram only) from processed unigram(removd context-denpend/indendent words)\n",
    "2. get 200 meaningful bigrams\n",
    "3. combine the 1 & 2 to get full vocabulary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get process unigram and vocab form 'unigramToVocab()'\n",
    "unigramList, final_cleaned_vocab = unigramToVocab()\n",
    "\n",
    "# get 200 meaningful word from 'genVocab()'\n",
    "target_bigrams = genVocab()\n",
    "\n",
    "# get final vocabulary \n",
    "final_vocab = list(unigramList)\n",
    "# combine the 200 bigrams\n",
    "final_vocab.extend(target_bigrams)\n",
    "\n",
    "# dictionary to store vocabulary index\n",
    "word_vocab_index = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wrtie final Vocabulary to .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_vocab():\n",
    "    # reomve any protential duplicate value\n",
    "    new_word_list = set(final_vocab)\n",
    "    result = list(new_word_list)\n",
    "    # sort vocabulary words\n",
    "    result.sort(reverse=False)\n",
    "\n",
    "    path = './31268102_vocab.txt'\n",
    "    vocab_text = ''\n",
    "    # reformat the words with index number\n",
    "    for index, item in enumerate(result):\n",
    "        # formatting with \": index\" \n",
    "        temp = item + ':' + str(index)\n",
    "        if not vocab_text:\n",
    "            vocab_text = temp\n",
    "        else:\n",
    "            vocab_text = vocab_text + '\\n' + temp\n",
    "        # detect bigrames\n",
    "        # reassgin the bigrame index(prevent all unigrams in front fo the file)\n",
    "        if '_' in item:\n",
    "            word_list = item.split('_')\n",
    "            word_vocab_index[word_list[0]] = index\n",
    "            word_vocab_index[word_list[1]] = index\n",
    "        else:\n",
    "            word_vocab_index[item] = index\n",
    "            \n",
    "    # write to .txt file\n",
    "    saved(vocab_text, path)\n",
    "\n",
    "# run function \n",
    "sample_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generatting vector\n",
    "11. Generate matrix through `final_cleaned_vocab{}` and write vector to `.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVect():\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", token_pattern=regexpStr)\n",
    "    # get data from final clean vocabulary\n",
    "    data_features = vectorizer.fit_transform([' '.join(value) for value in final_cleaned_vocab.values()])\n",
    "    vocabTwo = vectorizer.get_feature_names()\n",
    "    firstArray = data_features.toarray()\n",
    "    # get date form vocabulary\n",
    "    dateList = [date for date in final_cleaned_vocab.keys()]\n",
    "    \n",
    "    countVectText = ''\n",
    "    for index, date in enumerate(dateList):\n",
    "        temp = str(date)\n",
    "        for word, count in zip(vocabTwo, firstArray[index]):\n",
    "            if count > 0:\n",
    "                wordIndex = word_vocab_index.get(word)\n",
    "                temp = temp + ',' + str(wordIndex) + \":\" + str(count)\n",
    "        if not countVectText:\n",
    "            countVectText = temp + '\\n'\n",
    "        else:\n",
    "            countVectText = countVectText + temp + '\\n'\n",
    "\n",
    "    path = './31268102_countVec.txt'\n",
    "    saved(countVectText, path)\n",
    "\n",
    "\n",
    "getVect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
